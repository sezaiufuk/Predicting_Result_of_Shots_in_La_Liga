{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "Our problem is a classification problem, our target is whether a shot is goal or not.\n",
    "\n",
    "# Where Can We Use This Model In Real World ?\n",
    "\n",
    "We can use this model to analyze player performances, understand player habits on the field, determining which players to change in game or find out in advance whether a new player will be compatible with the team or not during transfer seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Dataset Over R from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas2ri.activate()\n",
    "ro.r('''\n",
    "        library(\"worldfootballR\")\n",
    "        laliga <- load_understat_league_shots(league = \"La liga\")\n",
    "     ''')\n",
    "laliga = pandas2ri.rpy2py(ro.r['laliga'])\n",
    "laliga.drop('league', axis=1, inplace=True)\n",
    "data = laliga[(laliga['date'] > '2022-01-01') & (laliga['date'] < '2024-06-10')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use La Liga dataset between 2023-01-01 and 2024-06-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulations and Fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem about NaN values and duplicate features, we are going to fix these problems by manipulating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def fixDataNaN(df):\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        df = ro.conversion.py2rpy(df)\n",
    "pairs = [['x','X'],['y','Y'],['x_g','xG'],['h_a','home_away'],['shot_type','shotType'],['last_action','lastAction']]\n",
    "\n",
    "def camel_case_columns(df):\n",
    "    def camel_case(column_name):\n",
    "        parts = column_name.split('_')\n",
    "        return str(parts[0] + ''.join(x.title() for x in parts[1:]))\n",
    "    \n",
    "    new_columns = []\n",
    "    for column in df.columns:\n",
    "        if '_' in column:\n",
    "            new_columns.append(camel_case(column))\n",
    "        else:\n",
    "            new_columns.append(str(column))\n",
    "    \n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "def fixMergeColumns(dataList, pairs):\n",
    "    for targetData in dataList:\n",
    "        for pair in pairs:\n",
    "            if pair[0] in targetData.columns and pair[1] in targetData.columns:\n",
    "                targetData['{}'.format(pair[1])].fillna(targetData['{}'.format(pair[0])], inplace=True)\n",
    "                targetData.drop(columns=['{}'.format(pair[0])], inplace=True)\n",
    "        targetData = camel_case_columns(targetData)\n",
    "        fixDataNaN(targetData)\n",
    "\n",
    "fixMergeColumns([data], pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train our model to predict whether the position ends up to a goal or not so we need to convert our goal and not goal situations to binary tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {\n",
    "    'Goal': 'Goal',\n",
    "    'BlockedShot': 'No Goal',\n",
    "    'MissedShots': 'No Goal',\n",
    "    'SavedShot': 'No Goal',\n",
    "    'ShotOnPost': 'No Goal',\n",
    "    'OwnGoal': 'No Goal'\n",
    "}\n",
    "\n",
    "data['result'] = pd.DataFrame(data['result'].map(replacement_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head(2)\n",
    "print(data['result'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable\n",
    "\n",
    "Our target variable is \"result\", this feature represents whether a shot is a goal or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Variables\n",
    "\n",
    "Our feature variables are going to help our model to learn and predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.drop('result',axis=1).columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalancedness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has imbalancedness problem which might cause our model to learn features of majority target variables better than the minority target variables, it might cause inaccurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['result'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('result', axis=1)\n",
    "Y = data['result']\n",
    "print(Y.head(2))\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "data = pd.concat([pd.DataFrame(Y),pd.DataFrame(X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shuffled = data.sample(frac=1, random_state=42)\n",
    "half_length = len(data_shuffled) // 2\n",
    "df_half1 = data_shuffled.iloc[:half_length]\n",
    "df_half2 = data_shuffled.iloc[half_length:]\n",
    "\n",
    "data = df_half1\n",
    "validation_data = df_half2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use SMOTE method for oversampling to fix the gap between minority and majority target variables. This method uses clustering methods to create new observations based on original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['result'] = data['result'].map({'Goal': 1, 'No Goal': 0})\n",
    "validation_data['result'] = validation_data['result'].map({'Goal': 1, 'No Goal': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation = validation_data['result']\n",
    "x_validation = validation_data.drop('result', axis=1)\n",
    "\n",
    "X = pd.DataFrame(data.drop('result', axis=1))\n",
    "Y = pd.DataFrame(data['result'])\n",
    "print(data.result.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "0    10478\n",
      "1    10478\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sm = SMOTE(k_neighbors=200,random_state=42,n_jobs=-1)\n",
    "x_res, y_res = sm.fit_resample(X, Y)\n",
    "\n",
    "# concat the resampled data\n",
    "data_res = pd.concat([pd.DataFrame(y_res), pd.DataFrame(x_res)], axis=1)\n",
    "print(data_res.result.value_counts())\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_res = scaler.fit_transform(x_res)\n",
    "\n",
    "X_validation = pd.DataFrame(scaler.fit_transform(x_validation))\n",
    "\n",
    "Y_validation = pd.DataFrame(validation_data['result'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_res, y_res, test_size=0.25, random_state=42, stratify=y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the best model to use is Random Forest after comparing Decision Tree, Logistic Regression,XGBoost and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-Fold Cross Validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(model, x_train, y_train, cv=5, scoring='balanced_accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test Accuracy Comparison to Check Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to have learned train set perfectly, we might have suspected of overfitting much more than the current situation if the test accuracies were bad but accuracy and balanced accuracy scores on train and test sets are very close to each other. Our model is good to go, but we are going to check if hyperparameter tuning or automl could improve our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, model.predict(x_train))\n",
    "train_balanced_accuracy = balanced_accuracy_score(y_train, model.predict(x_train))\n",
    "train_confusion_matrix = confusion_matrix(y_train, model.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9646879175415155\n",
      "Train Accuracy: 1.0\n",
      "\n",
      "Test Balanced Accuracy: 0.964684965125667\n",
      "Train Balanced Accuracy: 1.0\n",
      "\n",
      "Test Confusion Matrix: \n",
      "[[2568   52]\n",
      " [ 133 2486]]\n",
      "\n",
      "Train Confusion Matrix: \n",
      "[[7858    0]\n",
      " [   0 7859]]\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: {}\\nTrain Accuracy: {}\\n'.format(test_accuracy, train_accuracy))\n",
    "print('Test Balanced Accuracy: {}\\nTrain Balanced Accuracy: {}\\n'.format(test_balanced_accuracy, train_balanced_accuracy))\n",
    "print('Test Confusion Matrix: \\n{}\\n\\nTrain Confusion Matrix: \\n{}'.format(test_confusion_matrix, train_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Halving Random Search method and 5-Fold Cross Validation together to find the best hyperparameters, evaluating our iterations by balanced accuracy scores.\n",
    "\n",
    "The Halving Random Search method uses an elimination system as its base idea: the best of the two compared hyperparameter sets rises above on the leaderboard, and we get the best hyperparameters after the final round. Another pro of Halving Random Search is that it is much faster than Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=5000, num=10)],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [int(x) for x in np.linspace(2, 50)],\n",
    "    'min_samples_split': [int(x) for x in np.linspace(2, 5)],\n",
    "    'min_samples_leaf': [int(x) for x in np.linspace(2, 5)],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "halving = HalvingRandomSearchCV(model, param_grid, factor=3, resource='n_samples', max_resources=1000, random_state=42, verbose=0,scoring='balanced_accuracy', n_jobs=-1)\n",
    "halving.fit(x_train, y_train)\n",
    "print(\"Best Params:{}/nBest Balanced Accuracy:{}\".format(halving.best_params_,halving.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not get what we wanted from hyperparameter tuning, our balanced accuracy decreased and it might not be ideal to consume more time to random searching, so let's see how AutoML is going to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h2o.H2OFrame(pd.concat([pd.DataFrame(x_res), pd.DataFrame(y_res)], axis=1))\n",
    "\n",
    "train_hf, test_hf = hf.split_frame(ratios=[0.75], seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML\n",
    "\n",
    "We are going to use the H2O library because of its ease of use. We are going to set the maximum runtime to 300 seconds so our process won't consume too much time. AutoML is going to use 5-Fold Cross Validation by default and compare balanced accuracies and more metrics to choose the best model for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(max_models = 20,\n",
    "                balance_classes=True,\n",
    "\t\tseed =1,max_runtime_secs=600,verbosity='none')\n",
    "\n",
    "aml.train(training_frame = train_hf, y = 'result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML found the best model to be Gradient Boosting Machines. This algorithm is based on the idea of creating an ensemble of weak learners, typically decision trees, in a sequential manner. Each new model attempts to correct the errors of the previous models.\n",
    "\n",
    "Our new model's performance is almost the same as vanilla Random Forest. However, AutoML checked more metrics to validate that this is the right model for our data. Both models are black box models, so it does not affect us which one we choose for the sake of interpretability. We can trust more in the one that AutoML found for its generalizability because it checked more metrics when building the model. So, we are good to go now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "preds = aml.leader.predict(test_hf)\n",
    "\n",
    "y_test_gbm = test_hf['result'].as_data_frame().values.flatten()\n",
    "y_pred_gbm = preds['predict'].as_data_frame().values.flatten()\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test_gbm, y_pred_gbm)\n",
    "print(\"Balanced Accuracy Score: \", balanced_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the Model\n",
    "\n",
    "We are going to test our model with a new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "validation_x_hf = h2o.H2OFrame(pd.DataFrame(X_validation))\n",
    "validation_y_hf = h2o.H2OFrame(pd.DataFrame(Y_validation))\n",
    "\n",
    "                               \n",
    "preds_validation = aml.leader.predict(validation_x_hf)\n",
    "\n",
    "y_validation_test_gbm = validation_y_hf['result'].as_data_frame().values.flatten()\n",
    "y_validation_pred_gbm = preds_validation['predict'].as_data_frame().values.flatten()\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_validation_test_gbm, y_validation_pred_gbm)\n",
    "print(\"Balanced Accuracy Score: \", balanced_acc)\n",
    "\n",
    "print(confusion_matrix(y_validation_test_gbm, y_validation_pred_gbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2         3         4         5     6         7     \\\n",
      "0  0.834998  0.447619  0.871717  0.494867  0.057366  0.631198   1.0  0.973904   \n",
      "\n",
      "   8         9     ...  2500  2501  2502  2503  2504  2505  2506  2507  2508  \\\n",
      "0   0.0  0.428571  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   2509  \n",
      "0   1.0  \n",
      "\n",
      "[1 rows x 2510 columns]\n",
      "[[10286   221]\n",
      " [  871   304]]\n",
      "0.6188449038027333\n"
     ]
    }
   ],
   "source": [
    "y_validation_pred = model.predict(X_validation)\n",
    "\n",
    "print(confusion_matrix(Y_validation, y_validation_pred))\n",
    "\n",
    "print(balanced_accuracy_score(Y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
